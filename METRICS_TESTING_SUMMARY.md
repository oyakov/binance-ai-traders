# Binance Data Collection and Storage Services - Metrics Testing Summary

## Overview
This document summarizes the metrics configuration and testing setup for the Binance Data Collection and Storage services.

## Services Configuration

### 1. Binance Data Collection Service
- **Port**: 8086 (mapped from container port 8080)
- **Configuration**: `binance-data-collection/src/main/resources/application-testnet.yml`
- **Metrics Class**: `DataCollectionMetrics.java`
- **Dependencies**: âœ… Spring Boot Actuator + Micrometer Prometheus

#### Metrics Exposed:
- **Counters**:
  - `binance_data_collection_kline_events_received_total`
  - `binance_data_collection_kline_events_sent_kafka_total`
  - `binance_data_collection_kline_events_failed_kafka_total`
  - `binance_data_collection_websocket_connections_established_total`
  - `binance_data_collection_websocket_connections_failed_total`
  - `binance_data_collection_websocket_connections_closed_total`
  - `binance_data_collection_rest_api_calls_total`
  - `binance_data_collection_rest_api_calls_failed_total`

- **Gauges**:
  - `binance_data_collection_active_websocket_connections`
  - `binance_data_collection_active_kline_streams`
  - `binance_data_collection_last_kline_event_timestamp`
  - `binance_data_collection_last_websocket_connection_timestamp`

- **Timers**:
  - `binance_data_collection_kline_event_processing_duration_seconds`
  - `binance_data_collection_kafka_send_duration_seconds`
  - `binance_data_collection_websocket_connection_duration_seconds`
  - `binance_data_collection_rest_api_call_duration_seconds`

### 2. Binance Data Storage Service
- **Port**: 8087 (mapped from container port 8081)
- **Configuration**: `binance-data-storage/src/main/resources/application-testnet.yml`
- **Metrics Class**: `DataStorageMetrics.java`
- **Dependencies**: âœ… Spring Boot Actuator + Micrometer Prometheus

#### Metrics Exposed:
- **Counters**:
  - `binance_data_storage_kline_events_received_total`
  - `binance_data_storage_kline_events_saved_total`
  - `binance_data_storage_kline_events_failed_total`
  - `binance_data_storage_kline_events_compensated_total`
  - `binance_data_storage_postgres_saves_total`
  - `binance_data_storage_postgres_save_failures_total`
  - `binance_data_storage_elasticsearch_saves_total`
  - `binance_data_storage_elasticsearch_save_failures_total`
  - `binance_data_storage_kafka_consumer_errors_total`

- **Gauges**:
  - `binance_data_storage_active_kafka_consumers`
  - `binance_data_storage_last_kline_event_timestamp`
  - `binance_data_storage_last_successful_save_timestamp`
  - `binance_data_storage_postgres_connection_status`
  - `binance_data_storage_elasticsearch_connection_status`

- **Timers**:
  - `binance_data_storage_kline_event_processing_duration_seconds`
  - `binance_data_storage_postgres_save_duration_seconds`
  - `binance_data_storage_elasticsearch_save_duration_seconds`
  - `binance_data_storage_kafka_consumer_processing_duration_seconds`

## Endpoints Available

### Collection Service (Port 8086)
- **Health**: `http://localhost:8086/actuator/health`
- **Metrics**: `http://localhost:8086/actuator/metrics`
- **Prometheus**: `http://localhost:8086/actuator/prometheus`
- **Info**: `http://localhost:8086/actuator/info`

### Storage Service (Port 8087)
- **Health**: `http://localhost:8087/actuator/health`
- **Metrics**: `http://localhost:8087/actuator/metrics`
- **Prometheus**: `http://localhost:8087/actuator/prometheus`
- **Info**: `http://localhost:8087/actuator/info`

## Testing Scripts Created

### 1. `check-metrics.ps1`
- Simple verification script that checks configuration files
- Validates metrics classes exist
- Confirms dependencies are present
- âœ… **Status**: Working

### 2. `test-collection-storage-metrics.ps1`
- Comprehensive testing script for both services
- Tests all endpoints (health, metrics, prometheus, info)
- Validates specific metrics are available
- Includes retry logic and error handling
- **Usage**: `.\test-collection-storage-metrics.ps1 -StartServices -TestMetrics`

## Configuration Status

### âœ… Verified Working:
- [x] Collection service has proper Prometheus configuration
- [x] Storage service has comprehensive management endpoints enabled
- [x] Both services have required dependencies (Actuator + Micrometer)
- [x] Metrics classes are properly implemented with comprehensive monitoring
- [x] Docker configuration exposes correct ports
- [x] Health checks are configured

### ðŸ”§ Ready for Testing:
- [ ] Start Docker Desktop
- [ ] Run services: `docker-compose -f docker-compose-testnet.yml up -d binance-data-collection-testnet binance-data-storage-testnet`
- [ ] Test endpoints with: `.\test-collection-storage-metrics.ps1 -TestMetrics`

## Expected Behavior

When the services are running, you should see:

1. **Health endpoints** return status information
2. **Metrics endpoints** return JSON with all available metrics
3. **Prometheus endpoints** return metrics in Prometheus format
4. **Info endpoints** return application information

The metrics will be populated as the services process data:
- Collection service metrics will increase as it receives and processes kline data
- Storage service metrics will increase as it saves data to PostgreSQL and Elasticsearch

## Next Steps

1. **Start Docker Desktop** if not already running
2. **Start the services** using docker-compose
3. **Run the test script** to verify all endpoints are working
4. **Monitor metrics** to ensure they're being populated correctly
5. **Set up Prometheus/Grafana** for visualization if needed

## Troubleshooting

If services fail to start:
- Check Docker Desktop is running
- Verify all dependencies are available
- Check logs: `docker-compose -f docker-compose-testnet.yml logs binance-data-collection-testnet`
- Check logs: `docker-compose -f docker-compose-testnet.yml logs binance-data-storage-testnet`

If metrics are not appearing:
- Verify services are running and healthy
- Check that metrics classes are being instantiated
- Ensure the services are actually processing data
- Check for any errors in the application logs
